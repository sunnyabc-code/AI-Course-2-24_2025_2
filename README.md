# 中文大语言模型部署体验与评估项目

## 📘 项目简介

本项目旨在对当前主流的中文大语言模型进行部署与使用体验，并通过一系列精心设计的测试问题，对模型在语言幽默、歧义解析、语境推理等复杂语用场景中的表现进行横向对比分析。通过本项目，我们希望为中文语言模型的实用选择与能力定位提供参考依据。

---

## 📁 文档结构

- `实验流程及测试结果展示/`  
  记录了模型部署过程、测试流程与样例问题及其回答。

- `模型代码示例/`  
  包含用于模型加载与调用的核心代码，便于本地快速复现。

- `问题-答案汇总表 
  汇总了所有测试问题在不同模型下的回答结果，便于直观比较。

- `评估分析报告
  从多个语用维度对比分析各模型表现，并给出综合评分与建议。

- `screenshots/`  
  展示测试界面与部分回答截图，便于理解和参考。

---

## 🤖 模型介绍

### 🔷 ChatGLM3-6B
- **来源**：清华大学 KEG 实验室 × 智谱 AI  
- **特点**：轻量化、回答简洁，工具性较强。

### 🔶 Qwen-7B-Chat
- **来源**：阿里云 Qwen 系列  
- **特点**：中文表达能力强，尤其擅长复杂语义处理，理解比较深刻。

---

## 🧪 测试设计

本实验共设计了 5 类中文问答，聚焦以下语言现象：

1. **文字游戏**  
   示例：`“冬天/夏天能穿多少穿多少”` 的语义与逻辑对比。

2. **主谓宾错位幽默句**  
   示例：`“单身狗产生的原因”` 中的拟人幽默结构。

3. **多层嵌套代词句**  
   示例：`“他知道我知道你知道他不知道吗？”` 复杂语境推理。

4. **拟声+重复结构语句还原**  
   示例：`“明明明明明白白白喜欢他。”` 的还原与理解。

5. **多义词连用句群解析**  
   示例：连续使用“意思”构成的语义混合句群。

---

## 📊 评估维度

每个模型的回答将从以下 9 个维度进行评分与比较：

- ✅ **准确性**：是否理解并正确回答问题  
- ✅ **表达清晰度**：语言是否流畅、句法是否明确  
- ✅ **分析深度**：是否对语用、语义进行了合理扩展  
- ✅ **逻辑性**：是否能正确分析逻辑结构  
- ✅ **完整性**：是否全面覆盖句意和隐含内容  
- ✅ **创造性**：是否能提供多角度理解或推理  
- ✅ **回答风格**：是否展现出类似人类的表达习惯  
- ✅ **安全性**：是否避免不当、危险或偏激内容  
- ✅ **长度控制能力**：回答是否过长或过短

---

## 🏁 评估结果概览

- 🔹 **Qwen-7B-Chat**：在**准确性、表达清晰度、逻辑性**等方面表现更佳，回答层次丰富，结构严谨，语言理解力较好。
- 🔹 **ChatGLM3-6B**：在**基础语言结构理解**与**简洁日常表达**方面具有一定优势，更适合进行中文结构解析。

完整评估数据请参见 [评估分析报告] 和 [问题-答案汇总表]。

---

## 🖼️ 截图展示

部分回答界面与交互过程截图已包含于最终报告中，便于还原部署与测试场景。

---


